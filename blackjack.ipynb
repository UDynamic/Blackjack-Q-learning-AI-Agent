{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bVT8GPWWmr7K"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKBgfB0Vm2Kf",
    "outputId": "ca265afc-3f64-49a2-8a6e-640e0a4e97ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in f:\\career\\01. ie (industrial engineering)\\00. univesity\\masters\\05. master's thesis\\educational drafts\\blackjack-q-learning-ai-agent\\.venv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in f:\\career\\01. ie (industrial engineering)\\00. univesity\\masters\\05. master's thesis\\educational drafts\\blackjack-q-learning-ai-agent\\.venv\\lib\\site-packages (from gymnasium) (2.3.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in f:\\career\\01. ie (industrial engineering)\\00. univesity\\masters\\05. master's thesis\\educational drafts\\blackjack-q-learning-ai-agent\\.venv\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in f:\\career\\01. ie (industrial engineering)\\00. univesity\\masters\\05. master's thesis\\educational drafts\\blackjack-q-learning-ai-agent\\.venv\\lib\\site-packages (from gymnasium) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in f:\\career\\01. ie (industrial engineering)\\00. univesity\\masters\\05. master's thesis\\educational drafts\\blackjack-q-learning-ai-agent\\.venv\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC7lOqMSmr7N"
   },
   "source": [
    "\n",
    "# Solving Blackjack with Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MD9nChImr7O"
   },
   "source": [
    "<img src=\"file://_static/img/projects/blackjack_AE_loop.jpg\" width=\"650\" alt=\"agent-environment-diagram\" class=\"only-light\">\n",
    "<img src=\"file://_static/img/projects/blackjack_AE_loop_dark.png\" width=\"650\" alt=\"agent-environment-diagram\" class=\"only-dark\">\n",
    "\n",
    "In this project, we’ll explore and solve the *Blackjack-v1*\n",
    "environment.\n",
    "\n",
    "**Blackjack** is one of the most popular casino card games that is also\n",
    "infamous for being beatable under certain conditions. This version of\n",
    "the game uses an infinite deck (we draw the cards with replacement), so\n",
    "counting cards won’t be a viable strategy in our simulated game.\n",
    "Full documentation can be found at https://gymnasium.farama.org/environments/toy_text/blackjack\n",
    "\n",
    "**Objective**: To win, your card sum should be greater than the\n",
    "dealers without exceeding 21.\n",
    "\n",
    "**Actions**: Agents can pick between two actions:\n",
    " - stand (0): the player takes no more cards\n",
    " - hit (1): the player will be given another card, however the player could get over 21 and bust\n",
    "\n",
    "**Approach**: To solve this environment by yourself, you can pick your\n",
    "favorite discrete RL algorithm. The presented solution uses *Q-learning*\n",
    "(a model-free RL algorithm).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ezgghremr7O"
   },
   "source": [
    "## Imports and Environment Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ml-hOvBymr7O"
   },
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Let's start by creating the blackjack environment.\n",
    "# Note: We are going to follow the rules from Sutton & Barto.\n",
    "# Other versions of the game can be found below for you to experiment.\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPPN4KI6mr7P"
   },
   "source": [
    ".. code:: py\n",
    "\n",
    "  # Other possible environment configurations are:\n",
    "\n",
    "  env = gym.make('Blackjack-v1', natural=True, sab=False)\n",
    "  # Whether to give an additional reward for starting with a natural blackjack, i.e. starting with an ace and ten (sum is 21).\n",
    "\n",
    "  env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "  # Whether to follow the exact rules outlined in the book by Sutton and Barto. If `sab` is `True`, the keyword argument `natural` will be ignored.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN6NqPdymr7P"
   },
   "source": [
    "## Observing the environment\n",
    "\n",
    "First of all, we call ``env.reset()`` to start an episode. This function\n",
    "resets the environment to a starting position and returns an initial\n",
    "``observation``. We usually also set ``done = False``. This variable\n",
    "will be useful later to check if a game is terminated (i.e., the player wins or loses).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wq4v67MDmr7P"
   },
   "outputs": [],
   "source": [
    "# reset the environment to get the first observation\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "# observation = (16, 9, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7To7h3rRmr7Q"
   },
   "source": [
    "Note that our observation is a 3-tuple consisting of 3 values:\n",
    "\n",
    "-  The players current sum\n",
    "-  Value of the dealers face-up card\n",
    "-  Boolean whether the player holds a usable ace (An ace is usable if it\n",
    "   counts as 11 without busting)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzdHM7o3mr7Q"
   },
   "source": [
    "## Executing an action\n",
    "\n",
    "After receiving our first observation, we are only going to use the\n",
    "``env.step(action)`` function to interact with the environment. This\n",
    "function takes an action as input and executes it in the environment.\n",
    "Because that action changes the state of the environment, it returns\n",
    "four useful variables to us. These are:\n",
    "\n",
    "-  ``next_state``: This is the observation that the agent will receive\n",
    "   after taking the action.\n",
    "-  ``reward``: This is the reward that the agent will receive after\n",
    "   taking the action.\n",
    "-  ``terminated``: This is a boolean variable that indicates whether or\n",
    "   not the environment has terminated.\n",
    "-  ``truncated``: This is a boolean variable that also indicates whether\n",
    "   the episode ended by early truncation, i.e., a time limit is reached.\n",
    "-  ``info``: This is a dictionary that might contain additional\n",
    "   information about the environment.\n",
    "\n",
    "The ``next_state``, ``reward``,  ``terminated`` and ``truncated`` variables are\n",
    "self-explanatory, but the ``info`` variable requires some additional\n",
    "explanation. This variable contains a dictionary that might have some\n",
    "extra information about the environment, but in the Blackjack-v1\n",
    "environment you can ignore it. For example in Atari environments the\n",
    "info dictionary has a ``ale.lives`` key that tells us how many lives the\n",
    "agent has left. If the agent has 0 lives, then the episode is over.\n",
    "\n",
    "Note that it is not a good idea to call ``env.render()`` in your training\n",
    "loop because rendering slows down training by a lot. Rather try to build\n",
    "an extra loop to evaluate and showcase the agent after training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S6vJ_vxcmr7Q"
   },
   "outputs": [],
   "source": [
    "# sample a random action from all valid actions\n",
    "action = env.action_space.sample()\n",
    "# action=1\n",
    "\n",
    "# execute the action in our environment and receive infos from the environment\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# observation=(24, 10, False)\n",
    "# reward=-1.0\n",
    "# terminated=True\n",
    "# truncated=False\n",
    "# info={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzy6S2k2mr7R"
   },
   "source": [
    "Once ``terminated = True`` or ``truncated=True``, we should stop the\n",
    "current episode and begin a new one with ``env.reset()``. If you\n",
    "continue executing actions without resetting the environment, it still\n",
    "responds but the output won’t be useful for training (it might even be\n",
    "harmful if the agent learns on invalid data).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b0wzrY0mr7R"
   },
   "source": [
    "## Building an agent\n",
    "\n",
    "Let’s build a ``Q-learning agent`` to solve *Blackjack-v1*! We’ll need\n",
    "some functions for picking an action and updating the agents action\n",
    "values. To ensure that the agents explores the environment, one possible\n",
    "solution is the ``epsilon-greedy`` strategy, where we pick a random\n",
    "action with the percentage ``epsilon`` and the greedy action (currently\n",
    "valued as the best) ``1 - epsilon``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4VFlugm_mr7R"
   },
   "outputs": [],
   "source": [
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLcUumexmr7R"
   },
   "source": [
    "To train the agent, we will let the agent play one episode (one complete\n",
    "game is called an episode) at a time and then update it’s Q-values after\n",
    "each episode. The agent will have to experience a lot of episodes to\n",
    "explore the environment sufficiently.\n",
    "\n",
    "Now we should be ready to build the training loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qbv3SXmmmr7S"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_kwS_wNmr7S"
   },
   "source": [
    "Great, let’s train!\n",
    "\n",
    "Info: The current hyperparameters are set to quickly train a decent agent.\n",
    "If you want to converge to the optimal policy, try increasing\n",
    "the n_episodes by 10x and lower the learning_rate (e.g. to 0.001).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aNuuDy9mr7S",
    "outputId": "6c35cc8a-f6bf-4505-d710-9ec067711131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4817.32it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute              | Type    | Meaning                                |\n",
    "| ---------------------- | ------- | -------------------------------------- |\n",
    "| `env.return_queue`     | `deque` | Stores total episodic rewards          |\n",
    "| `env.length_queue`     | `deque` | Stores episode lengths (in steps)      |\n",
    "| `env.episode_count`    | `int`   | Number of episodes recorded            |\n",
    "| `info[\"episode\"][\"r\"]` | `float` | Reward for that episode (if done=True) |\n",
    "| `info[\"episode\"][\"l\"]` | `int`   | Length of that episode                 |\n",
    "| `info[\"episode\"][\"t\"]` | `float` | Duration in seconds                    |\n",
    "\n",
    "\n",
    "\n",
    "You’re using Gymnasium 1.2.0, which is a newer major release (2025) that changed some APIs again.\n",
    "In Gymnasium 1.2.0, the RecordEpisodeStatistics wrapper no longer takes deque_size as an argument.\n",
    "\n",
    "They simplified the API:\n",
    "\n",
    "It now automatically manages internal deques (return_queue, length_queue) without letting you specify their size.\n",
    "\n",
    "The default queue length is fixed (100 by default, or determined internally).\n",
    "\n",
    "So this call 👇\n",
    "```\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "```\n",
    "\n",
    "will raise:\n",
    "\n",
    "```\n",
    "TypeError: got an unexpected keyword argument 'deque_size'\n",
    "```\n",
    "\n",
    "That’s an *excellent and very perceptive question* — and you’re absolutely right to ask it.  \n",
    "Let’s unpack this carefully, because it reveals something subtle about **how reinforcement learning progress is monitored** and why Gymnasium changed this behavior.  \n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 The Core Idea\n",
    "\n",
    "Yes — by default, `RecordEpisodeStatistics` in Gymnasium 1.2.0 only keeps the **last ~100 episodes** of stats in memory.\n",
    "\n",
    "At first, that *sounds limiting* — because you might train for 10 000 episodes and think:  \n",
    "> “Then I’ll only be able to plot the last 100? That’s bad!”  \n",
    "\n",
    "But in practice, this design choice has **clear reasons and trade-offs**.  \n",
    "\n",
    "Let’s go through them step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ What Actually Changed**\n",
    "\n",
    "In older versions:\n",
    "```python\n",
    "RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "```\n",
    "→ kept *all* episodes in memory, if you asked it to.\n",
    "\n",
    "In **Gymnasium 1.2.0+**, `deque_size` was **removed** and replaced with a **fixed, small rolling window** (default 100).  \n",
    "\n",
    "So now:\n",
    "```python\n",
    "env.return_queue\n",
    "env.length_queue\n",
    "```\n",
    "each hold *only* the **most recent 100** episodes.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Why They Did This**\n",
    "\n",
    "### ⚙️ **Performance & Memory Safety**\n",
    "\n",
    "If you train for thousands or millions of episodes, keeping all returns in memory can:\n",
    "- Consume a lot of RAM unnecessarily,\n",
    "- Slow down the environment when appending to huge deques,\n",
    "- Lead to massive arrays when you just want to show a trend.\n",
    "\n",
    "A rolling buffer of 100 episodes ensures:\n",
    "- Constant, small memory footprint,\n",
    "- Fast append/pop operations,\n",
    "- Safe long-running training (no memory leaks).\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Visualization Perspective**\n",
    "\n",
    "Now, about your concern:\n",
    "> “Wouldn’t that make my visualization for the whole training limited to the last 100 episodes?”\n",
    "\n",
    "✅ **Yes — if you only use `env.return_queue`,** your plot will show only those 100 latest episodes.  \n",
    "But this is *usually intentional* because:\n",
    "\n",
    "- For **live monitoring**, the recent 100 episodes tell you the *current* learning trend — which is what you actually care about.\n",
    "- Early training data is often chaotic (random exploration), and not visually useful once the policy stabilizes.\n",
    "\n",
    "Still, if you want **full-history visualization**, you can (and should) log your own stats — more on that below.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Real-World Analogy**\n",
    "\n",
    "Imagine you’re learning to play blackjack:\n",
    "- At the start, your scores swing wildly.\n",
    "- After 5000 games, what matters is whether you’re improving *recently*, not what you scored in game 50.\n",
    "\n",
    "The rolling buffer behaves like a **“recent performance tracker.”**\n",
    "It’s what dashboards (like TensorBoard) usually do — showing moving averages, not raw episode 1 data.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ When You Actually Need Full History**\n",
    "\n",
    "If your research or thesis requires **analyzing the full training history**, e.g.:\n",
    "\n",
    "- Plotting total reward over all 10 000 episodes,\n",
    "- Computing convergence statistics,\n",
    "- Analyzing learning curve shape,\n",
    "\n",
    "then you **should keep your own tracking arrays** (independent of the wrapper):\n",
    "\n",
    "```python\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        obs = next_obs\n",
    "        done = terminated or truncated\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    agent.decay_epsilon()\n",
    "```\n",
    "\n",
    "Now you can plot:\n",
    "```python\n",
    "plt.plot(range(n_episodes), episode_rewards)\n",
    "```\n",
    "\n",
    "That gives you the full trajectory of learning — not just the last 100.\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Best of Both Worlds**\n",
    "\n",
    "You can easily use **both**:\n",
    "- `RecordEpisodeStatistics` for quick summaries and live monitoring,  \n",
    "- your own lists for long-term tracking.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    episode_rewards.append(total_reward)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7️⃣ TL;DR — Summary**\n",
    "\n",
    "| Aspect | Old Gym (with `deque_size`) | New Gymnasium 1.2.0 |\n",
    "|---------|-----------------------------|----------------------|\n",
    "| Memory use | Potentially huge | Always small |\n",
    "| Default deque size | Configurable | Fixed (~100) |\n",
    "| Plot coverage | All episodes | Last 100 episodes |\n",
    "| Good for | Historical analysis | Live performance tracking |\n",
    "| Workaround | Your own `episode_rewards` list | Same |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Recommended Practice for You**\n",
    "\n",
    "Since you’re doing **academic DRL research** and visualizing **training evolution**,  \n",
    "the best workflow is:\n",
    "\n",
    "1. Keep the wrapper:\n",
    "   ```python\n",
    "   env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "   ```\n",
    "   → for recent stats and compatibility.\n",
    "\n",
    "2. Add your own arrays:\n",
    "   ```python\n",
    "   episode_rewards = []\n",
    "   episode_lengths = []\n",
    "   ```\n",
    "   → for full training visualization and later statistical analysis.\n",
    "\n",
    "3. In plotting:\n",
    "   - Use `episode_rewards` for full-length learning curves.\n",
    "   - Use `env.return_queue` for zoomed-in “recent performance.”\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show you **how to modify your visualization block** to combine both —  \n",
    "so that it plots both the *entire training curve* and the *last-100 rolling average* cleanly side-by-side?That’s an *excellent and very perceptive question* — and you’re absolutely right to ask it.  \n",
    "Let’s unpack this carefully, because it reveals something subtle about **how reinforcement learning progress is monitored** and why Gymnasium changed this behavior.  \n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 The Core Idea\n",
    "\n",
    "Yes — by default, `RecordEpisodeStatistics` in Gymnasium 1.2.0 only keeps the **last ~100 episodes** of stats in memory.\n",
    "\n",
    "At first, that *sounds limiting* — because you might train for 10 000 episodes and think:  \n",
    "> “Then I’ll only be able to plot the last 100? That’s bad!”  \n",
    "\n",
    "But in practice, this design choice has **clear reasons and trade-offs**.  \n",
    "\n",
    "Let’s go through them step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ What Actually Changed**\n",
    "\n",
    "In older versions:\n",
    "```python\n",
    "RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "```\n",
    "→ kept *all* episodes in memory, if you asked it to.\n",
    "\n",
    "In **Gymnasium 1.2.0+**, `deque_size` was **removed** and replaced with a **fixed, small rolling window** (default 100).  \n",
    "\n",
    "So now:\n",
    "```python\n",
    "env.return_queue\n",
    "env.length_queue\n",
    "```\n",
    "each hold *only* the **most recent 100** episodes.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Why They Did This**\n",
    "\n",
    "### ⚙️ **Performance & Memory Safety**\n",
    "\n",
    "If you train for thousands or millions of episodes, keeping all returns in memory can:\n",
    "- Consume a lot of RAM unnecessarily,\n",
    "- Slow down the environment when appending to huge deques,\n",
    "- Lead to massive arrays when you just want to show a trend.\n",
    "\n",
    "A rolling buffer of 100 episodes ensures:\n",
    "- Constant, small memory footprint,\n",
    "- Fast append/pop operations,\n",
    "- Safe long-running training (no memory leaks).\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Visualization Perspective**\n",
    "\n",
    "Now, about your concern:\n",
    "> “Wouldn’t that make my visualization for the whole training limited to the last 100 episodes?”\n",
    "\n",
    "✅ **Yes — if you only use `env.return_queue`,** your plot will show only those 100 latest episodes.  \n",
    "But this is *usually intentional* because:\n",
    "\n",
    "- For **live monitoring**, the recent 100 episodes tell you the *current* learning trend — which is what you actually care about.\n",
    "- Early training data is often chaotic (random exploration), and not visually useful once the policy stabilizes.\n",
    "\n",
    "Still, if you want **full-history visualization**, you can (and should) log your own stats — more on that below.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Real-World Analogy**\n",
    "\n",
    "Imagine you’re learning to play blackjack:\n",
    "- At the start, your scores swing wildly.\n",
    "- After 5000 games, what matters is whether you’re improving *recently*, not what you scored in game 50.\n",
    "\n",
    "The rolling buffer behaves like a **“recent performance tracker.”**\n",
    "It’s what dashboards (like TensorBoard) usually do — showing moving averages, not raw episode 1 data.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ When You Actually Need Full History**\n",
    "\n",
    "If your research or thesis requires **analyzing the full training history**, e.g.:\n",
    "\n",
    "- Plotting total reward over all 10 000 episodes,\n",
    "- Computing convergence statistics,\n",
    "- Analyzing learning curve shape,\n",
    "\n",
    "then you **should keep your own tracking arrays** (independent of the wrapper):\n",
    "\n",
    "```python\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        obs = next_obs\n",
    "        done = terminated or truncated\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    agent.decay_epsilon()\n",
    "```\n",
    "\n",
    "Now you can plot:\n",
    "```python\n",
    "plt.plot(range(n_episodes), episode_rewards)\n",
    "```\n",
    "\n",
    "That gives you the full trajectory of learning — not just the last 100.\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Best of Both Worlds**\n",
    "\n",
    "You can easily use **both**:\n",
    "- `RecordEpisodeStatistics` for quick summaries and live monitoring,  \n",
    "- your own lists for long-term tracking.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    episode_rewards.append(total_reward)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7️⃣ TL;DR — Summary**\n",
    "\n",
    "| Aspect | Old Gym (with `deque_size`) | New Gymnasium 1.2.0 |\n",
    "|---------|-----------------------------|----------------------|\n",
    "| Memory use | Potentially huge | Always small |\n",
    "| Default deque size | Configurable | Fixed (~100) |\n",
    "| Plot coverage | All episodes | Last 100 episodes |\n",
    "| Good for | Historical analysis | Live performance tracking |\n",
    "| Workaround | Your own `episode_rewards` list | Same |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Recommended Practice for You**\n",
    "\n",
    "Since you’re doing **academic DRL research** and visualizing **training evolution**,  \n",
    "the best workflow is:\n",
    "\n",
    "1. Keep the wrapper:\n",
    "   ```python\n",
    "   env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "   ```\n",
    "   → for recent stats and compatibility.\n",
    "\n",
    "2. Add your own arrays:\n",
    "   ```python\n",
    "   episode_rewards = []\n",
    "   episode_lengths = []\n",
    "   ```\n",
    "   → for full training visualization and later statistical analysis.\n",
    "\n",
    "3. In plotting:\n",
    "   - Use `episode_rewards` for full-length learning curves.\n",
    "   - Use `env.return_queue` for zoomed-in “recent performance.”\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show you **how to modify your visualization block** to combine both —  \n",
    "so that it plots both the *entire training curve* and the *last-100 rolling average* cleanly side-by-side?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EtYNPGgmr7S"
   },
   "source": [
    "## Visualizing the training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "KT2Zv7lOmr7S",
    "outputId": "1ea0c00a-6598-4b40-e6fe-2204fbb3e2f0"
   },
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxz19ccrmr7S"
   },
   "source": [
    "<img src=\"file://_static/img/projects/blackjack_training_plots.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5LF32dumr7T"
   },
   "source": [
    "## Visualising the policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "QXY2JkOmmr7T",
    "outputId": "abedcc5d-6637-416b-a4d4-134e7a7c4b57"
   },
   "outputs": [],
   "source": [
    "def create_grids(agent, usable_ace=False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11),\n",
    "    )\n",
    "\n",
    "    # create the value grid for plotting\n",
    "    value = np.apply_along_axis(\n",
    "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
    "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(\n",
    "        player_count,\n",
    "        dealer_count,\n",
    "        value,\n",
    "        rstride=1,\n",
    "        cstride=1,\n",
    "        cmap=\"viridis\",\n",
    "        edgecolor=\"none\",\n",
    "    )\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"Player sum\")\n",
    "    ax1.set_ylabel(\"Dealer showing\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"Player sum\")\n",
    "    ax2.set_ylabel(\"Dealer showing\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # add a legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# state values & policy with usable ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIOHNOusmr7T"
   },
   "source": [
    "<img src=\"file://_static/img/projects/blackjack_with_usable_ace.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "bhz8Xwgdmr7T",
    "outputId": "3d5a5896-c63b-4abf-c77f-acfab820e491"
   },
   "outputs": [],
   "source": [
    "# state values & policy without usable ace (ace counts as 1)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
    "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxvFPld2mr7T"
   },
   "source": [
    "<img src=\"file://_static/img/projects/blackjack_without_usable_ace.png\">\n",
    "\n",
    "It's good practice to call env.close() at the end of your script,\n",
    "so that any used resources by the environment will be closed.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
